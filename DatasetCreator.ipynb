{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "labels_mapping = {\n",
    "        'Education':  ['Education', 'Required Education'],\n",
    "        'Certification':   ['Certification','Required Certification'],\n",
    "        'Qualifications':   ['Qualifications', 'Required Qualifications'],\n",
    "        'WorkExperience': ['Work Experience', 'Required Work  Experience', 'work', 'Experience Level', 'Experience'],\n",
    "        'TechnicalSkills':   ['Technical Skills', 'Technical'],\n",
    "        'SoftSkill':    ['Required Soft Skill','Soft Skill','Required Soft','Required_Soft','Soft_Skill', 'soft'],\n",
    "        'HardSkill': ['Required Hard Skill','Soft Skill','Required Hard','Required_Hard','Soft_Hard','hard','Hard'],\n",
    "        'Skills': ['Skills'],\n",
    "        'Proficiencies':  ['Proficiencies', 'Required Proficiencies'], \n",
    "        'Benefits': ['Benefits'],\n",
    "        'CompanyCulture': ['Company Culture/Values', 'company_culture', 'Company Values', 'Company Culture', 'Company_values','Company', 'Company_values'],\n",
    "        'qualifications': ['Requirement'],\n",
    "        \"Product\" : [\"product\"],\n",
    "        \"Service\" : [\"service\"],\n",
    "        \"Upcoming\" : [\"upcoming\"],\n",
    "        \"JobDuties\" : [\"jobDuties\", \"duties\"],\n",
    "    }  \n",
    "keynames = [\n",
    "     ['Education','Required Education'],\n",
    "    ['Certification','Required Certification'],\n",
    "    ['Qualifications', 'Required Qualifications'],\n",
    "    ['Work Experience', 'Required Work Experience', 'work', 'Experience Level', 'Experience'],\n",
    "     ['Technical Skills', 'Technical'],\n",
    "     ['Required Hard Skill','Soft Skill','Required Hard','Required_Hard','Soft_Hard','hard'],\n",
    "     ['Required Soft Skill','Soft Skill','Required Soft','Required_Soft','Soft_Skill', 'soft'],\n",
    "    ['Skill'],\n",
    "     ['Proficiencies', 'Required Proficiencies'], \n",
    "     ['Company Culture/Values', 'Company Values', 'Company Culture', 'Company_values','Company', 'Company_values'],\n",
    "     ['Benefits'], [\"product\"],  [\"service\"],  [\"upcoming\"],  [\"jobDuties\", \"job\", \"duties\"]\n",
    "]  \n",
    "def parse_missformed_json(text):\n",
    "    parsed_data = {}\n",
    "    current_key = None\n",
    "    current_array = None\n",
    "    for line in text.split('\\n'):\n",
    "        match_key = re.match(r'\\s*\"([^\"]*)\"\\s*:', line)\n",
    "        if match_key:\n",
    "            #print(match_key)\n",
    "            current_key = match_key.group(1)\n",
    "            if any(current_key.lower() in map(str.lower, sublist) for sublist in keynames):\n",
    "                #print(current_key)\n",
    "                current_array = [] \n",
    "                matches = re.findall(r'\"([^\"]*)\"', line)\n",
    "                #print(\"matches\",matches) \n",
    "                for value in matches[1:]:\n",
    "                    if any(value.lower() in map(str.lower, sublist) for sublist in keynames):\n",
    "                        continue\n",
    "                    #print(value)\n",
    "                    current_array.append(value)\n",
    "                   \n",
    "                parsed_data[current_key] = current_array\n",
    "    return parsed_data\n",
    " \n",
    "\n",
    "def get_label(key):  \n",
    "    for category, keywords in labels_mapping.items(): \n",
    "     \n",
    "        if any(keyword.lower() in key.lower() for keyword in keywords):\n",
    "            return category \n",
    "    return \"\"\n",
    "  \n",
    "def extract_json_from_text(text):\n",
    "    # Find the index of the first '{' character\n",
    "    start_index = text.find('{')\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    # Find the index of the last '}' character\n",
    "    end_index = text.rfind('}')\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "    \n",
    "    # Extract the substring between the first '{' and last '}' characters\n",
    "    json_text = text[start_index:end_index+1]\n",
    "    \n",
    "    return json_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "\n",
    "jobs_posting_file_path = r'./job_postings.csv'\n",
    "df = pd.read_csv(jobs_posting_file_path)\n",
    "rawdata = []\n",
    "filename = r\"./data.json\"   \n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    rawdata.extend(json.load(file)) \n",
    " \n",
    "print(len(rawdata)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the raw text in the data.json into actual JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_data = []\n",
    "err = 0\n",
    "for item in rawdata:\n",
    "    sentence = item.get('description', item.get('text', ''))\n",
    "    try:\n",
    "        text = extract_json_from_text(item['output'])\n",
    "        try: \n",
    "            all_data.append({\"sentence\":sentence, \"output\" : json.loads(text) })   \n",
    "        except Exception as e:    \n",
    "            print(e)\n",
    "            text = extract_json_from_text(text)\n",
    "            all_data.append({\"sentence\":sentence, \"output\" : json.loads(text) })   \n",
    "            pass\n",
    "    except Exception as e:   \n",
    "        try:     \n",
    "            text = extract_json_from_text(item['output'])\n",
    "            out = parse_missformed_json(text) \n",
    "            all_data.append({\"sentence\": sentence , \"output\" :  out  })   \n",
    "        except Exception as e:   \n",
    "            print(e)\n",
    "            pass\n",
    "            err += 1 \n",
    "        pass\n",
    "    \n",
    "print(len(all_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive traverse the json objects and try to get the label and it's values.\n",
    "Not all the JSON data is formated the same way due to the LLM nature, but doing this I can capture almost 100Â¤ of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm   \n",
    "\n",
    "def gather_data_with_label(data, get_label, item): \n",
    "    all_items = []\n",
    "\n",
    "    def traverse_and_gather(data,item):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                label = get_label(key)\n",
    "                if label != \"\" and isinstance(value, list) and len(value) > 1:\n",
    "                    for list_item in value:\n",
    "                        if isinstance(list_item, str):\n",
    "                            all_items.append({\"sentence\": item['sentence'], \"label\": label, \"output\": list_item, \"item\": item}) \n",
    "                traverse_and_gather(value,item)  \n",
    "\n",
    "        elif isinstance(data, list):\n",
    "            for item2 in data:\n",
    "                traverse_and_gather(item2,item)  \n",
    "\n",
    "    traverse_and_gather(data,item)\n",
    "    return all_items\n",
    "\n",
    "all_raw_items = [] \n",
    "for item in all_data:\n",
    "    extracted_data = gather_data_with_label(item['output'], get_label,item) \n",
    "    all_raw_items.extend(extracted_data) \n",
    "    \n",
    "print(len(all_raw_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data and remove data that might be in the wrong section or is just invalid such as if the LLM outputs \"Education: [ \"N/A\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "def filter_data(data): \n",
    "    invalids =set()\n",
    "    filtered_data = []\n",
    "    missing_values = [ \"\", \", \", \". \" , \" ,\", \" \", \"not specified\"\n",
    "        \"n/a\", \"none\", \"na\", \"blank\", \"missing\", \"not applicable\", \n",
    "        \"not available\", \"undefined\", \"null\", \"empty\", \"not provided\", \n",
    "        \"no value\", \"unspecified\", \"unknown\", \"void\", \"-\", \"--\", \"nil\", \n",
    "        \"n.a.\", \"n.a\", \"nil\", \"not set\", \"not specified\", \"not given\", \n",
    "        \"no entry\", \"not entered\", \"not included\", \"not valid\", \n",
    "        \"not applicable\", \n",
    "    ] \n",
    "\n",
    "    for entryR in data:\n",
    "        entry = entryR['output']\n",
    "        \n",
    "        entry_processed = entry.strip().replace(\"_\", \" \").lower() \n",
    "        if entry_processed in missing_values or len(wordnet.synsets(entry_processed.replace(\" \",\"\"))) > 0 or len(entry_processed) < 2 or len(entry_processed.split(\" \")) <= 2:\n",
    "            invalids.add(entryR['output'])  \n",
    "        elif ' ' in entry_processed or len(wordnet.synsets(entry_processed)) == 0:\n",
    "            filtered_data.append(entryR) \n",
    "        else: \n",
    "            invalids.add(entryR['output']) \n",
    "            \n",
    "    return filtered_data,invalids\n",
    " \n",
    "filtered_items , invalids = filter_data(all_raw_items)\n",
    "print(len(filtered_items))\n",
    "print(len(invalids))\n",
    "\n",
    "forbidden_keywords_mapping = {\n",
    "  \"Education\": [\"degree\", \"diploma\", \"academic background\", \"university\"],\n",
    "  \"Qualifications\": [  \"accreditation\", \"qualification\"],\n",
    "  \"WorkExperience\": [\"work experience\", \"employment history\", \"professional experience\", \"previous positions\", \"job history\", \"career history\"],\n",
    "  \"TechnicalSkills\": [\"programming languages\", \"software tools\", \"hardware expertise\", \"technical competencies\", \"IT skills\"], \n",
    "  \"Certification\": [\"certification\", \"accreditation\", \"professional designation\", \"certificate\", \"license\"],\n",
    "  \"HardSkill\": [\"specific skills\", \"quantifiable skills\", \"job-specific skills\", \"technical abilities\", \"industry-specific skills\"],\n",
    "  \"Benefits\": [\"achievements\", \"awards\", \"contributions\", \"value\", \"benefits\", \"advantages\"],\n",
    "  \"CompanyCulture\": [\"values\", \"personality traits\", \"work style\", \"company culture fit\", \"team\", \"team player\",  \"organizational fit\"] \n",
    "}\n",
    "\n",
    "all_items = []\n",
    "for item in filtered_items:\n",
    "    found = False \n",
    "    if item['label'] in forbidden_keywords_mapping:\n",
    "        for key, value in forbidden_keywords_mapping.items():  \n",
    "            for labelVal in value:\n",
    "                if key != item['label'] and labelVal.lower() in  item['output'].lower() :\n",
    "                    found = True \n",
    "                    break  \n",
    "    if found == False:\n",
    "        all_items.append(item)\n",
    "\n",
    "for data in all_items: \n",
    "    if data['label']  == \"qualifications\":\n",
    "        data['label']  = \"Qualifications\"  \n",
    "                     \n",
    "print(len(all_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from FlagEmbedding import FlagReranker  ,BGEM3FlagModel\n",
    "import os \n",
    "reranker = FlagReranker(\"BAAI/bge-reranker-base\", use_fp16=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word_at_capital(word):\n",
    "    splits = [(i, c) for i, c in enumerate(word[1:], 1) if c.isupper()]\n",
    "    for index, letter in reversed(splits):\n",
    "        return word[:index], word[index:]\n",
    "\n",
    "def check_wordnet(words):\n",
    "    return all(len(wordnet.synsets(word)) > 0  for word in words)\n",
    "\n",
    "def find_words_split_at_capital(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    split_words = [split_word_at_capital(word) for word in words if any(c.isupper() for c in word)]\n",
    "    split_words = [split for split in split_words if split]\n",
    "    return [split for split in split_words if all(len(word) >= 3 for word in split) and check_wordnet(split)]\n",
    "\n",
    "def insert_newlines(text):\n",
    "    split_words = find_words_split_at_capital(text)\n",
    "    for split in split_words:\n",
    "        old_word = ''.join(split)\n",
    "        new_word = '\\n'.join(split)\n",
    "        text = text.replace(old_word, new_word)\n",
    "    return text \n",
    "\n",
    "def split_text(text): \n",
    "    chunks = re.findall(r'(.*?(?:\\.|\\n|$))', text)\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "def chunk_text(text): \n",
    "    chunks = split_text(insert_newlines(text))  \n",
    "    chunks = [chunk for chunk in chunks if len(chunk) <= 256]\n",
    "    return chunks \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = {}\n",
    "for item in all_items:\n",
    "    age = item['sentence'] \n",
    "    if age == \"\":\n",
    "        continue\n",
    "    if age not in grouped_data:\n",
    "        grouped_data[age] = []   \n",
    "    grouped_data[age].append(item)\n",
    "print(len(grouped_data))\n",
    "     \n",
    "reRankerDatas = [] \n",
    "\n",
    "for sentence, grouped in grouped_data.items(): \n",
    "    group_list = list(grouped)  \n",
    "    rowData = []\n",
    "    nonLabels = []\n",
    "    leftOvers = list(group_list)   \n",
    "     \n",
    "    for chunk in chunk_text(sentence):\n",
    "        spacy_format = { 'text' : chunk, 'annotations' : []}\n",
    "        if len(chunk) > 3: \n",
    "            chunk_data = []\n",
    "            for data in group_list:\n",
    "                if data['output'].lower() in chunk.lower(): \n",
    "                    chunk_data.append(data) \n",
    "                    if data in leftOvers:\n",
    "                        leftOvers.remove(data) \n",
    "                    \n",
    "            if chunk_data:\n",
    "                for data in chunk_data:\n",
    "                    spacy_format = {'text': chunk, 'data': data}\n",
    "                    rowData.append(spacy_format) \n",
    "            else: \n",
    "                nonLabels.append(chunk)        \n",
    "            \n",
    "    if len(rowData) > 0 and len(nonLabels) > 5:   \n",
    "        for data in rowData:  \n",
    "            if len(data['data']['output']) > 256:\n",
    "                continue\n",
    "            pairs = []\n",
    "            for nonLabel in nonLabels:\n",
    "                pairs.append([data['text'],  nonLabel])\n",
    "                 \n",
    "            scores = reranker.compute_score(pairs) \n",
    "            sorted_pairs = sorted(zip(scores, pairs))\n",
    " \n",
    "            top_pairs = sorted_pairs[:3]   \n",
    "            nonSelected = []   \n",
    "            for score, pair in sorted_pairs[:3]: \n",
    "                nonSelected.append(pair[1])  \n",
    "                \n",
    "            reRankerDatas.append( { \"query\": data['data']['label'] , \"pos\" : [ data['text']], \"neg\" : nonSelected})\n",
    "            reRankerDatas.append( { \"query\": data['data']['label'] , \"pos\" : [ data['data']['output']], \"neg\" : nonSelected}) \n",
    "            \n",
    "            validLefts = []\n",
    "            for leftover in validLefts:\n",
    "                if leftover['label'] == data['data']['label'] and len(leftover['output']) < 256: \n",
    "                    reRankerDatas.append( { \"query\": data['data']['label'] , \"pos\" : [ leftover['output'] ], \"neg\" : nonSelected})    \n",
    "         \n",
    "                \n",
    "            validLefts.append(data['data']['output'])\n",
    "            reRankerDatas.append( { \"query\": \"Breadtext\" , \"pos\" : nonSelected, \"neg\" : validLefts }) \n",
    "            reRankerDatas.append( { \"query\": data['data']['label'] , \"pos\" :validLefts, \"neg\" : nonSelected})\n",
    "            \n",
    "print(len(reRankerDatas))      \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have too little data or not distinct enough data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "oppesiteMap = {\n",
    "    'WorkExperience' : ['Education','CompanyCulture','Benefits','Breadtext'],\n",
    "    'Education' : [ 'CompanyCulture', 'WorkExperience','Benefits','Breadtext'],\n",
    "    'CompanyCulture' : ['Education','WorkExperience','Benefits','Breadtext'],\n",
    "    'Certification'  : ['Education','CompanyCulture','Benefits','Breadtext'],\n",
    "    'Product'  : ['Education','CompanyCulture','Benefits','Breadtext'],\n",
    "    'Service'  : ['Education','CompanyCulture','Benefits','Breadtext'],\n",
    "    'JobDuties'  : ['Education','CompanyCulture','Benefits','Breadtext'],\n",
    "    'Upcoming'  : ['Education','CompanyCulture','Benefits','Breadtext']\n",
    "} \n",
    "neg_data = {}\n",
    "for data in reRankerDatas:\n",
    "    for key, value in oppesiteMap.items():\n",
    "        if data['query'] in value:\n",
    "            if key not in neg_data:\n",
    "                neg_data[key] = []\n",
    "            neg_data[key].extend(data['pos'])\n",
    "            \n",
    "new_data = []            \n",
    "for item in reRankerDatas: \n",
    "    negs = random.sample(neg_data[item['label']], 3)\n",
    "    new_data.append({ \"query\": item['label'] , \"pos\" : [ item['output']], \"neg\" : negs}) \n",
    "\n",
    "reRankerDatas.extend(new_data) \n",
    "print(len(reRankerDatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Format the data that might help reranker infer the meaning behind the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import copy\n",
    "  \n",
    "lowercase_category_mapping = {\n",
    "    'Education': 'education', \n",
    "    'Certification': 'certification', \n",
    "    'Qualifications': 'qualifications', \n",
    "    'WorkExperience': 'work experience', \n",
    "    'TechnicalSkills': 'hard skills', \n",
    "    'SoftSkill': 'soft skills', \n",
    "    'HardSkill': 'hard skills', \n",
    "    'Skills': 'hard skills', \n",
    "    'Proficiencies': 'qualifications', \n",
    "    'Benefits': 'benefits', \n",
    "    \"Breadtext\": 'breadtext',\n",
    "    'CompanyCulture': 'company culture',  \n",
    "    \"Product\": 'product',\n",
    "    \"Service\": 'service',\n",
    "    \"Upcoming\": 'upcoming offerings',\n",
    "    \"JobDuties\": 'job duties',\n",
    "}\n",
    " \n",
    "copyDatas = copy.copy(reRankerDatas)    \n",
    "for data in copyDatas:\n",
    "    if isinstance(data['pos'], str):\n",
    "        data['pos'] = [data['pos']]\n",
    "    \n",
    "    for item in data['pos']:\n",
    "        item = item.replace(\"\\n\", \"\") \n",
    "        \n",
    "    for item in data['neg']:\n",
    "        item = item.replace(\"\\n\", \"\")    \n",
    "        \n",
    "    if \"Example of\" not in data['query']:\n",
    "        data['query'] =  lowercase_category_mapping[data['query']] \n",
    "            \n",
    "        data['query'] = \"Example of \" + data['query'].lower()\n",
    "          \n",
    "with open(\"reRanker_raw_train_dataset.json\", \"w\") as file:\n",
    "    for item in copyDatas:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(copyDatas))\n",
    "def remove_duplicates(json_list):\n",
    "    unique_json_objects = []\n",
    "    seen = set()\n",
    "    for json_obj in json_list:\n",
    "        json_str = json.dumps(json_obj, sort_keys=True)\n",
    "        if json_str not in seen:\n",
    "            unique_json_objects.append(json_obj)\n",
    "            seen.add(json_str)\n",
    "    return unique_json_objects\n",
    "\n",
    "unique_json_objects = remove_duplicates(copyDatas)\n",
    "print(len(unique_json_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit the breadtext so it doesn't overpower the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadtext_objects = [obj for obj in unique_json_objects if obj.get('query') == 'Example of breadtext']\n",
    "other_objects = [obj for obj in unique_json_objects if obj.get('query') !='Example of breadtext']\n",
    "\n",
    "random.shuffle(breadtext_objects)\n",
    "max_breadtext_objects = breadtext_objects[:15000]\n",
    "\n",
    "selected_objects = max_breadtext_objects + other_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reRanker_train_dataset_cleaned.json\", \"w\") as file:\n",
    "    for item in selected_objects:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    " \n",
    "queries = [obj['query'] for obj in selected_objects]\n",
    " \n",
    "query_counts = Counter(queries)\n",
    " \n",
    "unique_queries = list(query_counts.keys())\n",
    "counts = list(query_counts.values())\n",
    "             \n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(unique_queries, counts, color='skyblue')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Queries')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying the bar chart\n",
    "plt.show()\n",
    "query_counts = Counter(queries)\n",
    "\n",
    "# Displaying the results\n",
    "for query, count in query_counts.items():\n",
    "    print(f\"Query: {query}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After fine-tuning the reranker model, convert to ONNX if you want to use it in e.g .NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = r'./Reranker_finetuned'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "dummy_model_input = tokenizer([['what is panda?', 'hi']], padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    " \n",
    "# export\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple(dummy_model_input.values()),\n",
    "    f=\"torch-model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    \n",
    "    do_constant_folding=True, \n",
    "    opset_version=13, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
